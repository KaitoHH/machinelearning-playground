{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\katiohh\\anaconda3\\envs\\ml\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras as ks\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, SimpleRNN, Activation, LSTM, Reshape, Lambda, Dense\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique characters: len = 3757\n"
     ]
    }
   ],
   "source": [
    "filepath = 'data/chi_names.txt'\n",
    "data = open(filepath, encoding='utf-8').read().lower()\n",
    "uniq = set(data)\n",
    "ndim = len(uniq)\n",
    "print('unique characters: len =', ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2arr(name):\n",
    "    arr = np.zeros((len(name), ndim))\n",
    "    for i, ch in enumerate(name):\n",
    "        arr[i, ch2idx[ch]] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def arr2word(arr, showProb=False):\n",
    "    name = ''\n",
    "    prob = 1.0\n",
    "    for vec in arr:\n",
    "        ch = np.random.choice(ndim, p=vec)\n",
    "        if showProb:\n",
    "            print(vec[ch])\n",
    "        prob = vec[ch]\n",
    "        name += idx2ch[ch]\n",
    "    return name, prob\n",
    "\n",
    "import pickle\n",
    "def save_dict(filepath, dict_obj):\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(dict_obj, f)\n",
    "\n",
    "        \n",
    "def load_dict(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch2idx = {word: idx for idx, word in enumerate(uniq)}\n",
    "idx2ch = {idx: word for idx, word in enumerate(uniq)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "x (InputLayer)               (None, None, 3757)        0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, None, 128)         497408    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 3757)        484653    \n",
      "=================================================================\n",
      "Total params: 982,061\n",
      "Trainable params: 982,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden = 128\n",
    "rnn_cell = SimpleRNN(hidden, return_sequences=True)\n",
    "\n",
    "x = Input(shape=(None, ndim), name='x')\n",
    "out = rnn_cell(x)\n",
    "out = Dense(ndim, activation='softmax')(out)\n",
    "\n",
    "model = Model(x, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: traning model using variable length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:09 Time: 0:01:09\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 Time: 0:01:08\n",
      "100% (86716 of 86716) |###################| Elapsed Time: 0:01:08 ETA:  0:00:00"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "from random import shuffle\n",
    "import progressbar\n",
    "lines = open(filepath, encoding='utf-8').readlines()\n",
    "shuffle(lines)\n",
    "\n",
    "for epochs in range(20):\n",
    "    counter = 0\n",
    "    bar = progressbar.ProgressBar(max_value=len(lines))\n",
    "    for length, chunks in groupby(lines, key=len):\n",
    "        chk = list(chunks)\n",
    "        arr = np.zeros((len(chk), length, ndim), dtype=bool)\n",
    "        counter += len(chk)\n",
    "        for i, name in enumerate(chk):\n",
    "            for j, ch in enumerate(name):\n",
    "                arr[i, j, ch2idx[ch]] = 1\n",
    "        x = arr[:, :-1]\n",
    "        y = arr[:, 1:]\n",
    "        model.fit(x, y, batch_size=128, verbose=0)\n",
    "        bar.update(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: padding name to maxlength using '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_name(name, max_length):\n",
    "    name += '\\n' * (max_length - len(name))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "lines = open(filepath, encoding='utf-8').readlines()\n",
    "shuffle(lines)\n",
    "max_length = len(max(lines, key=len))\n",
    "lines = [padding_name(name, max_length) for name in lines]\n",
    "\n",
    "def batch_generator(lines, batch_size=128):\n",
    "    chunks = [lines[i:i + batch_size] for i in range(0, len(lines), batch_size)]\n",
    "    while True:\n",
    "        for chunk in chunks:\n",
    "            arr = np.zeros((batch_size, max_length, ndim))\n",
    "            for i, name in enumerate(chunk):\n",
    "                for j, ch in enumerate(name):\n",
    "                    arr[i, j, ch2idx[ch]] = 1\n",
    "            x = arr[:, :-1]\n",
    "            y = arr[:, 1:]\n",
    "            yield (x, y)\n",
    "\n",
    "model.fit_generator(generator=batch_generator(lines), steps_per_epoch=len(lines)/128, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n",
    "Since Python use different random seed for hash function in every session, in order to reload trained model, you cannot save only model itself, but also ch2idx, idx2ch object as well because these 2 objects use set function to generate dict object.\n",
    "\n",
    "See also https://stackoverflow.com/a/27522708/7620214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model/chi_name.h5')\n",
    "\n",
    "# save dict\n",
    "save_dict('model/ch2idx.pkl', ch2idx)\n",
    "save_dict('model/idx2ch.pkl', idx2ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# load model\n",
    "model = load_model('model/chi_name.h5')\n",
    "\n",
    "# load dict\n",
    "ch2idx = load_dict('model/ch2idx.pkl')\n",
    "idx2ch = load_dict('model/idx2ch.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(first_name):\n",
    "    name = first_name\n",
    "    last_prob = 1\n",
    "    for i in range(1, 10):\n",
    "        next = word2arr(name)\n",
    "        next = model.predict(next.reshape(1, i, ndim))\n",
    "        ch, prob = arr2word(next.reshape(i, ndim)[-1].reshape(1, ndim))\n",
    "        if ch[0] is '\\n':\n",
    "            break\n",
    "        name += ch[0]\n",
    "        last_prob *= prob\n",
    "    return name, last_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('陈开静', 3.2350599947714447e-06)\n",
      "('陈进俊', 0.00010984689275285626)\n",
      "('陈欣志', 2.845677842833465e-05)\n",
      "('陈芳林', 2.1386588258407238e-05)\n",
      "('陈晓海', 0.00025052245772494083)\n",
      "('陈福忠', 1.5073073087405915e-05)\n",
      "('陈青', 0.0016482708742842078)\n",
      "('陈民', 0.0007179697277024388)\n",
      "('陈中锦', 2.071043745415803e-05)\n",
      "('陈天', 0.014639856293797493)\n",
      "('张国翠', 5.243099708932865e-05)\n",
      "('张博', 0.0016613940242677927)\n",
      "('张慧', 0.009406695142388344)\n",
      "('张敏萍', 0.00010950164997166473)\n",
      "('张万', 0.0007539856596849859)\n",
      "('张明安', 0.0001086827391844885)\n",
      "('张', 1)\n",
      "('张世会', 8.934337866905616e-06)\n",
      "('张晓建', 0.00021921020577242434)\n",
      "('张春旺', 7.235907900231037e-06)\n",
      "('王武明', 1.2880774324599718e-06)\n",
      "('王建刚', 1.5832891328023345e-05)\n",
      "('王小', 0.012258147820830345)\n",
      "('王秀', 0.012613345868885517)\n",
      "('王芳', 0.006224652286618948)\n",
      "('王龙', 0.005746682174503803)\n",
      "('王', 1)\n",
      "('王兴', 0.012211103923618793)\n",
      "('王华', 0.01619190350174904)\n",
      "('王莹君', 3.6088496159969783e-07)\n",
      "('赵柏文', 2.577176772160304e-06)\n",
      "('赵凤贵', 4.4025703446773595e-06)\n",
      "('赵', 1)\n",
      "('赵家双', 0.00017261608799236036)\n",
      "('赵广永', 9.779513713119997e-05)\n",
      "('赵兰宝', 1.791328366837464e-05)\n",
      "('赵敏海', 0.00021514907339663034)\n",
      "('赵福', 0.004673847928643227)\n",
      "('赵凤洪', 3.4862177255309476e-05)\n",
      "('赵卿', 0.00015809600881766528)\n",
      "('钱华', 0.03466038778424263)\n",
      "('钱金', 0.006521244999021292)\n",
      "('钱新', 0.006104494445025921)\n",
      "('钱春', 0.012994397431612015)\n",
      "('钱龙', 0.006585241295397282)\n",
      "('钱文涛', 7.969192198901184e-05)\n",
      "('钱红春', 0.00013639592960161453)\n",
      "('钱刚', 0.0028236466459929943)\n",
      "('钱景耀', 1.6872432762340636e-06)\n",
      "('钱正霖', 9.580504445689863e-07)\n",
      "('孙贤云', 6.854036803735988e-05)\n",
      "('孙', 1)\n",
      "('孙', 1)\n",
      "('孙才伟', 7.587185294134942e-06)\n",
      "('孙清花', 2.2367696803585017e-05)\n",
      "('孙', 1)\n",
      "('孙军洪', 0.00024792605602664414)\n",
      "('孙玲彦', 4.948138912995101e-06)\n",
      "('孙景', 0.0015410050982609391)\n",
      "('孙国义', 0.000108387436383541)\n",
      "('李孝奎', 6.189851826923971e-06)\n",
      "('李小洪', 0.00018733822764183117)\n",
      "('李', 1)\n",
      "('李朝靖', 4.953154619876965e-07)\n",
      "('李超贵', 3.801384016234935e-06)\n",
      "('李红旭', 0.00013074794150398812)\n",
      "('李', 1)\n",
      "('李亚跃', 3.6345889133459043e-05)\n",
      "('李', 1)\n",
      "('李海钦', 2.785127783723999e-06)\n",
      "('黄慧志', 0.00010204389276158299)\n",
      "('黄雪爱', 8.828313936024056e-05)\n",
      "('黄艳凤', 4.526001117733042e-05)\n",
      "('黄阳', 0.004108937922865152)\n",
      "('黄', 1)\n",
      "('黄彪锋', 1.441349074484801e-06)\n",
      "('黄冰长', 4.219138483491948e-06)\n",
      "('黄桂海', 0.00012044384589365853)\n",
      "('黄月兴', 1.0427059404194373e-05)\n",
      "('黄兴兴', 0.00010539661677460774)\n",
      "('周华', 0.029203111305832863)\n",
      "('周', 1)\n",
      "('周军洪', 0.00027524510441996955)\n",
      "('周江', 0.0031486705411225557)\n",
      "('周进', 0.00622681574895978)\n",
      "('周', 1)\n",
      "('周生锡', 4.980700462481336e-06)\n",
      "('周', 1)\n",
      "('周建正', 1.0708861832382701e-05)\n",
      "('周中', 0.003793389070779085)\n",
      "('杨', 1)\n",
      "('杨维', 0.005991770885884762)\n",
      "('杨雨超', 2.3156407505111774e-05)\n",
      "('杨文旭', 0.00018746757513041917)\n",
      "('杨', 1)\n",
      "('杨晓军', 0.0003958006380304635)\n",
      "('杨耀艳', 4.221937488491697e-05)\n",
      "('杨', 1)\n",
      "('杨晓国', 0.00021610127326680134)\n",
      "('杨勇全', 1.6262052498381904e-05)\n",
      "('何珍', 0.0026239249855279922)\n",
      "('何继丽', 5.571874092926625e-06)\n",
      "('何', 1)\n",
      "('何', 1)\n",
      "('何春', 0.012208676896989346)\n",
      "('何', 1)\n",
      "('何彪', 0.0023187173064798117)\n",
      "('何亚华', 0.0005251673349646532)\n",
      "('何凤', 0.0038271204102784395)\n",
      "('何生坤', 5.997605248157064e-07)\n",
      "('欧阳', 0.5439412593841553)\n",
      "('欧学', 0.0024012504145503044)\n",
      "('欧阳', 0.5439412593841553)\n",
      "('欧阳', 0.5439412593841553)\n",
      "('欧彪', 0.004752195905894041)\n",
      "('欧阳', 0.5439412593841553)\n",
      "('欧阳', 0.5439412593841553)\n",
      "('欧阳旭', 0.000835533570870961)\n",
      "('欧阳', 0.5439412593841553)\n",
      "('欧阳', 0.5439412593841553)\n",
      "('上华', 0.046331051737070084)\n",
      "('上', 1)\n",
      "('上', 1)\n",
      "('上', 1)\n",
      "('上', 1)\n",
      "('上', 1)\n",
      "('上', 1)\n",
      "('上林', 0.004457124974578619)\n",
      "('上', 1)\n",
      "('上军', 0.01039606798440218)\n"
     ]
    }
   ],
   "source": [
    "first_name = ['陈', '张', '王', '赵', '钱', '孙', '李', '黄', '周', '杨', '何', '欧', '上']\n",
    "for name in first_name:\n",
    "    for _ in range(10):\n",
    "        print(sample(name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
