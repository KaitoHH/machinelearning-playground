{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Masking, Bidirectional, Dropout, TimeDistributed\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from keras.callbacks import CSVLogger\n",
    "from gensim.models import KeyedVectors\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras as ks\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/msra/msra_train.txt'\n",
    "cv_path = 'data/msra/msra_testright.txt'\n",
    "test_path = 'data/msra/msra_testright.txt'\n",
    "wv_path = 'word2vec_model/news12g_bdbk20g_nov90g_dim128.bin'\n",
    "MAX_TEXT_LENGTH = 581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        sentences = []\n",
    "        sentences_ner = []\n",
    "        pos = []\n",
    "        sentence = ''\n",
    "        l = f.readlines()\n",
    "        for index, line in enumerate(l):\n",
    "            line = line.strip()\n",
    "            if len(line):\n",
    "                word, p = line.split()\n",
    "                sentence += word\n",
    "                pos.append(p)\n",
    "            else:\n",
    "                sentences.append(sentence)\n",
    "                sentences_ner.append(pos)\n",
    "                sentence = ''\n",
    "                pos = []\n",
    "        return sentences, sentences_ner\n",
    "\n",
    "\n",
    "def read_and_fit(path, word_tokenizer, pos_tokenizer):\n",
    "    sentences, pos = read_data(path)\n",
    "    word_tokenizer.fit_on_texts(sentences)\n",
    "    pos_tokenizer.fit_on_texts(pos)\n",
    "    return sentences, pos, word_tokenizer, pos_tokenizer\n",
    "\n",
    "\n",
    "def prepare_x(sentences, tokenizer):\n",
    "    seqs = tokenizer.texts_to_sequences(sentences)\n",
    "    seqs = pad_sequences(seqs, maxlen=MAX_TEXT_LENGTH)\n",
    "    return seqs\n",
    "\n",
    "\n",
    "def prepare_y(pos, tokenizer):\n",
    "    seqs = tokenizer.texts_to_sequences(pos)\n",
    "    pad = tokenizer.word_index['O']\n",
    "    seqs = pad_sequences(seqs, value=pad, maxlen=MAX_TEXT_LENGTH)\n",
    "    seqs = np.vectorize(lambda x: x - 1)(seqs)  # decrease 1 on every element\n",
    "    y = to_categorical(seqs)\n",
    "    return y\n",
    "\n",
    "\n",
    "def prepare_data(sentences, pos, word_tokenizer, pos_tokenizer):\n",
    "    return prepare_x(sentences, word_tokenizer), prepare_y(pos, pos_tokenizer)\n",
    "\n",
    "\n",
    "def preprocessing(train_path, cv_path=None, test_path=None):\n",
    "    word_tokenizer = Tokenizer(char_level=True)\n",
    "    pos_tokenizer = Tokenizer()\n",
    "\n",
    "    print('Reading train data...')\n",
    "    train_sens, train_pos, word_tokenizer, pos_tokenizer = read_and_fit(\n",
    "        train_path, word_tokenizer, pos_tokenizer)\n",
    "    if cv_path:\n",
    "        print('Reading cv data...')\n",
    "        cv_sens, cv_pos, word_tokenizer, pos_tokenizer = read_and_fit(\n",
    "            cv_path, word_tokenizer, pos_tokenizer)\n",
    "    if test_path:\n",
    "        print('Reading test data...')\n",
    "        test_sens, test_pos, word_tokenizer, pos_tokenizer = read_and_fit(\n",
    "            test_path, word_tokenizer, pos_tokenizer)\n",
    "\n",
    "    print('Preparing train data...')\n",
    "    x_train, y_train = prepare_data(train_sens, train_pos, word_tokenizer,\n",
    "                                    pos_tokenizer)\n",
    "    if cv_path:\n",
    "        print('Preparing cv data...')\n",
    "        x_cv, y_cv = prepare_data(cv_sens, cv_pos, word_tokenizer,\n",
    "                                  pos_tokenizer)\n",
    "    if test_path:\n",
    "        print('Preparing test data...')\n",
    "        x_test, y_test = prepare_data(test_sens, test_pos, word_tokenizer,\n",
    "                                      pos_tokenizer)\n",
    "\n",
    "    return (x_train, y_train, x_cv, y_cv, x_test, y_test, word_tokenizer,\n",
    "            pos_tokenizer)\n",
    "\n",
    "\n",
    "def make_embedding_matrix(wv, word_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 64))\n",
    "    for word, i in word_index.items():\n",
    "        if word in wv.vocab:\n",
    "            embedding_matrix[i] = wv.word_vec(word)\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def load_and_make_embedding_matrix(wv_path, word_index):\n",
    "    wv = KeyedVectors.load_word2vec_format(\n",
    "        'word2vec_model/news12g_bdbk20g_nov90g_dim128.bin', binary=True)\n",
    "    embedding_matrix = make_embedding_matrix(wv, word_index)\n",
    "    return embedding_matrix, wv\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=2).ravel()\n",
    "    y_p = np.argmax(y_pred, axis=2).ravel()\n",
    "    postive = np.sum(np.logical_and(y_t > 0, y_t == y_p))\n",
    "    tot = np.sum(y_t > 0)\n",
    "    return postive / tot\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=2).ravel()\n",
    "    y_p = np.argmax(y_pred, axis=2).ravel()\n",
    "    postive = np.sum(np.logical_and(y_t > 0, y_t == y_p))\n",
    "    tot = np.sum(y_p > 0)\n",
    "    return postive / tot\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=2).ravel()\n",
    "    y_p = np.argmax(y_pred, axis=2).ravel()\n",
    "    return np.sum(y_t == y_p) / len(y_t)\n",
    "\n",
    "def make_matrix(y_true, y_pred):\n",
    "    y_t = np.argmax(y_true, axis=2).ravel()\n",
    "    y_p = np.argmax(y_pred, axis=2).ravel()\n",
    "    return pd.crosstab(y_t, y_p,rownames=['0','1','2','3','4','5','6'], colnames=['Predict'])\n",
    "    \n",
    "\n",
    "def eval(y_true, y_pred):\n",
    "    rec = recall(y_true, y_pred)\n",
    "    acc = accuracy(y_true, y_pred)\n",
    "    pre = precision(y_true, y_pred)\n",
    "    f1 = 2 * rec * pre / (rec + pre)\n",
    "    print('precision:', pre,'recall:', rec, 'accuracy:', acc, 'f1:', f1)\n",
    "\n",
    "def test_sentences(sentences):\n",
    "    vec = word_tokenizer.texts_to_sequences([sentences])\n",
    "    vec = pad_sequences(vec, MAX_TEXT_LENGTH)\n",
    "    ret = model.predict([vec])[0][-len(sentences):]\n",
    "    ret = np.argmax(ret, axis=1) + 1\n",
    "    return [index_pos[x] for x in ret]\n",
    "\n",
    "\n",
    "def pretty_ner(sentences, ner_ret):\n",
    "    for x, y in zip(sentences, ner_ret):\n",
    "        print(x, y)\n",
    "\n",
    "def plot_confusiopdmatrix(df_confusion, title='Confusion matrix',\n",
    "                          cmap=plt.cm.RdBu_r):\n",
    "    df_confusion = df_confusion / df_confusion.sum(axis=1)\n",
    "#     for i in range(7):\n",
    "#         df_confusion[i][i]=0\n",
    "    from pylab import rcParams\n",
    "    rcParams['figure.figsize'] = 8, 6\n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, ['O','BP','IP','BL','IL','BO','IO'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['O','BP','IP','BL','IL','BO','IO'])\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n",
    "\n",
    "import pickle\n",
    "def save_obj(filename, obj):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "\n",
    "def load_obj(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train data...\n",
      "Reading cv data...\n",
      "Reading test data...\n",
      "Preparing train data...\n",
      "Preparing cv data...\n",
      "Preparing test data...\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train, x_cv, y_cv, x_test, y_test, word_tokenizer,\n",
    " pos_tokenizer) = preprocessing(\n",
    "     train_path=train_path,\n",
    "     cv_path=cv_path,\n",
    "     test_path=test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make embedding matrix\n",
    "API Doc: [gensim.models.keyedvectors.Word2VecKeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix, _ = load_and_make_embedding_matrix(\n",
    "    wv_path=wv_path,\n",
    "    word_index=word_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28145, 577) (28145, 577, 7)\n",
      "(2399, 577) (2399, 577, 7)\n",
      "(2399, 577) (2399, 577, 7)\n",
      "(4529, 64)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_cv.shape, y_cv.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 577, 64)           289856    \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, 577, 128)          66048     \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, 577, 128)          98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, 577, 128)          98816     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 577, 7)            903       \n",
      "_________________________________________________________________\n",
      "crf_13 (CRF)                 (None, 577, 7)            119       \n",
      "=================================================================\n",
      "Total params: 554,558\n",
      "Trainable params: 264,702\n",
      "Non-trainable params: 289,856\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=64,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_TEXT_LENGTH,\n",
    "        trainable=False,\n",
    "        mask_zero=True\n",
    "        ))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.5)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.5)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.5)))\n",
    "# model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Dense(7))  \n",
    "crf = CRF(7)\n",
    "model.add(crf)\n",
    "model.compile(\n",
    "    optimizer=ks.optimizers.Adadelta(),\n",
    "    loss=crf.loss_function,\n",
    "    metrics=[crf.accuracy])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 577, 64)           289856    \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 577, 128)          66048     \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 577, 7)            903       \n",
      "=================================================================\n",
      "Total params: 356,807\n",
      "Trainable params: 66,951\n",
      "Non-trainable params: 289,856\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=embedding_matrix.shape[0],\n",
    "        output_dim=64,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_TEXT_LENGTH,\n",
    "        trainable=False,\n",
    "        mask_zero=True\n",
    "        ))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True, recurrent_dropout=0.5)))\n",
    "model.add((Dense(7,activation='softmax')))\n",
    "# crf = CRF(7)\n",
    "# model.add(crf)\n",
    "model.compile(\n",
    "    optimizer=ks.optimizers.Adadelta(),\n",
    "    loss=ks.losses.categorical_crossentropy,\n",
    "    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "|Model|Recall|\n",
    "|-----|------|\n",
    "|LSTM|52.58%|\n",
    "|LSTM with masking|60.2%|\n",
    "|BiLSTM with masking|68%|\n",
    "|BiLSTM with masking+CRF|72%|\n",
    "|BiLSTM with masking+CRF+Dropout0.3|82.21%| Training on 100 epochs\n",
    "|BiLSTM with masking+CRF+Dropout0.5|83.29%| Training on 250 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark on test\n",
    "|Model|Precision|Recall|Accuracy|F1\n",
    "|-----|------|\n",
    "|LSTM + CRF|86.989|79.221|99.645|82.924|\n",
    "|BiLSTM with masking||||\n",
    "|BiLSTM with masking+CRF|88.234|83.436|99.701|85.768|\n",
    "|BiLSTM with masking+CRF+Dropout0.3|85.215|86.174|99.69|85.692|\n",
    "|BiLSTM with masking+CRF+Dropout0.5|86.970|85.934|99.709|86.450|\n",
    "|stacked BiLSTM + CRF|88.099|86.486|99.726|87.285|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark on dev\n",
    "|Model|Precision|Recall|Accuracy|F1\n",
    "|-----|------|\n",
    "|LSTM with masking + CRF|92.922|87.111|99.771|89.923|\n",
    "|BiLSTM with masking||||\n",
    "|BiLSTM with masking+CRF|99.075|97.358|99.957|98.209|\n",
    "|BiLSTM with masking+CRF+Dropout0.3|96.409|97.519|99.929|96.961|\n",
    "|BiLSTM with masking+CRF+Dropout0.5|97.183|96.997|99.934|97.090|\n",
    "|stacked BiLSTM + CRF|99.126|98.907|99.977|99.016|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "28145/28145 [==============================] - 745s 26ms/step - loss: 27.4555 - acc: 0.8340\n",
      "Epoch 2/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 27.2456 - acc: 0.8601\n",
      "Epoch 3/250\n",
      "28145/28145 [==============================] - 730s 26ms/step - loss: 27.1469 - acc: 0.8930\n",
      "Epoch 4/250\n",
      "28145/28145 [==============================] - 729s 26ms/step - loss: 27.0887 - acc: 0.9121\n",
      "Epoch 5/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 27.0519 - acc: 0.9227\n",
      "Epoch 6/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 27.0260 - acc: 0.9303\n",
      "Epoch 7/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 27.0075 - acc: 0.9352\n",
      "Epoch 8/250\n",
      "28145/28145 [==============================] - 727s 26ms/step - loss: 26.9916 - acc: 0.9404\n",
      "Epoch 9/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.9811 - acc: 0.9430\n",
      "Epoch 10/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 26.9717 - acc: 0.9457\n",
      "Epoch 11/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9642 - acc: 0.9477\n",
      "Epoch 12/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.9575 - acc: 0.9498\n",
      "Epoch 13/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9522 - acc: 0.9510\n",
      "Epoch 14/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.9462 - acc: 0.9536\n",
      "Epoch 15/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9422 - acc: 0.9549\n",
      "Epoch 16/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.9384 - acc: 0.9558\n",
      "Epoch 17/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.9349 - acc: 0.9568\n",
      "Epoch 18/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9309 - acc: 0.9589\n",
      "Epoch 19/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.9293 - acc: 0.9588\n",
      "Epoch 20/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.9258 - acc: 0.9610\n",
      "Epoch 21/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.9242 - acc: 0.9609\n",
      "Epoch 22/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9214 - acc: 0.9626\n",
      "Epoch 23/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.9196 - acc: 0.9636\n",
      "Epoch 24/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9178 - acc: 0.9640\n",
      "Epoch 25/250\n",
      "28145/28145 [==============================] - 727s 26ms/step - loss: 26.9167 - acc: 0.9643\n",
      "Epoch 26/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.9145 - acc: 0.9656\n",
      "Epoch 27/250\n",
      "28145/28145 [==============================] - 740s 26ms/step - loss: 26.9131 - acc: 0.9661\n",
      "Epoch 28/250\n",
      "28145/28145 [==============================] - 734s 26ms/step - loss: 26.9119 - acc: 0.9664\n",
      "Epoch 29/250\n",
      "28145/28145 [==============================] - 737s 26ms/step - loss: 26.9110 - acc: 0.9668\n",
      "Epoch 30/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 26.9095 - acc: 0.9676\n",
      "Epoch 31/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9082 - acc: 0.9687\n",
      "Epoch 32/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9082 - acc: 0.9678\n",
      "Epoch 33/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 26.9061 - acc: 0.9692\n",
      "Epoch 34/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.9057 - acc: 0.9697\n",
      "Epoch 35/250\n",
      "28145/28145 [==============================] - 727s 26ms/step - loss: 26.9049 - acc: 0.9696\n",
      "Epoch 36/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.9038 - acc: 0.9706\n",
      "Epoch 37/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.9024 - acc: 0.9715\n",
      "Epoch 38/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.9026 - acc: 0.9708\n",
      "Epoch 39/250\n",
      "28145/28145 [==============================] - 727s 26ms/step - loss: 26.9017 - acc: 0.9715\n",
      "Epoch 40/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.9006 - acc: 0.9720\n",
      "Epoch 41/250\n",
      "28145/28145 [==============================] - 757s 27ms/step - loss: 26.9000 - acc: 0.9723\n",
      "Epoch 42/250\n",
      "28145/28145 [==============================] - 769s 27ms/step - loss: 26.8995 - acc: 0.9728\n",
      "Epoch 43/250\n",
      "28145/28145 [==============================] - 748s 27ms/step - loss: 26.8989 - acc: 0.9728\n",
      "Epoch 44/250\n",
      "28145/28145 [==============================] - 747s 27ms/step - loss: 26.8977 - acc: 0.9735\n",
      "Epoch 45/250\n",
      "28145/28145 [==============================] - 767s 27ms/step - loss: 26.8971 - acc: 0.9742\n",
      "Epoch 46/250\n",
      "28145/28145 [==============================] - 770s 27ms/step - loss: 26.8971 - acc: 0.9738\n",
      "Epoch 47/250\n",
      "28145/28145 [==============================] - 776s 28ms/step - loss: 26.8962 - acc: 0.9744\n",
      "Epoch 48/250\n",
      "28145/28145 [==============================] - 787s 28ms/step - loss: 26.8956 - acc: 0.9751\n",
      "Epoch 49/250\n",
      "28145/28145 [==============================] - 753s 27ms/step - loss: 26.8956 - acc: 0.9747\n",
      "Epoch 50/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8949 - acc: 0.9749\n",
      "Epoch 51/250\n",
      "28145/28145 [==============================] - 769s 27ms/step - loss: 26.8943 - acc: 0.9753\n",
      "Epoch 52/250\n",
      "28145/28145 [==============================] - 774s 27ms/step - loss: 26.8938 - acc: 0.9759\n",
      "Epoch 53/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8933 - acc: 0.9760\n",
      "Epoch 54/250\n",
      "28145/28145 [==============================] - 919s 33ms/step - loss: 26.8931 - acc: 0.9762\n",
      "Epoch 55/250\n",
      "28145/28145 [==============================] - 950s 34ms/step - loss: 26.8924 - acc: 0.9765\n",
      "Epoch 56/250\n",
      "28145/28145 [==============================] - 943s 34ms/step - loss: 26.8923 - acc: 0.9765\n",
      "Epoch 57/250\n",
      "28145/28145 [==============================] - 930s 33ms/step - loss: 26.8914 - acc: 0.9774\n",
      "Epoch 58/250\n",
      "28145/28145 [==============================] - 854s 30ms/step - loss: 26.8908 - acc: 0.9777\n",
      "Epoch 59/250\n",
      "28145/28145 [==============================] - 828s 29ms/step - loss: 26.8908 - acc: 0.9773\n",
      "Epoch 60/250\n",
      "28145/28145 [==============================] - 851s 30ms/step - loss: 26.8903 - acc: 0.9780\n",
      "Epoch 61/250\n",
      "28145/28145 [==============================] - 806s 29ms/step - loss: 26.8904 - acc: 0.9781\n",
      "Epoch 62/250\n",
      "28145/28145 [==============================] - 783s 28ms/step - loss: 26.8896 - acc: 0.9782\n",
      "Epoch 63/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8897 - acc: 0.9781\n",
      "Epoch 64/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8895 - acc: 0.9782\n",
      "Epoch 65/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8883 - acc: 0.9794\n",
      "Epoch 66/250\n",
      "28145/28145 [==============================] - 760s 27ms/step - loss: 26.8885 - acc: 0.9786\n",
      "Epoch 67/250\n",
      "28145/28145 [==============================] - 729s 26ms/step - loss: 26.8879 - acc: 0.9793\n",
      "Epoch 68/250\n",
      "28145/28145 [==============================] - 729s 26ms/step - loss: 26.8882 - acc: 0.9791\n",
      "Epoch 69/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8871 - acc: 0.9801\n",
      "Epoch 70/250\n",
      "28145/28145 [==============================] - 736s 26ms/step - loss: 26.8872 - acc: 0.9797\n",
      "Epoch 71/250\n",
      "28145/28145 [==============================] - 734s 26ms/step - loss: 26.8868 - acc: 0.9801\n",
      "Epoch 72/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8865 - acc: 0.9802\n",
      "Epoch 73/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8869 - acc: 0.9796\n",
      "Epoch 74/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8859 - acc: 0.9804\n",
      "Epoch 75/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8859 - acc: 0.9808\n",
      "Epoch 76/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8862 - acc: 0.9800\n",
      "Epoch 77/250\n",
      "28145/28145 [==============================] - 735s 26ms/step - loss: 26.8851 - acc: 0.9812\n",
      "Epoch 78/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8848 - acc: 0.9816\n",
      "Epoch 79/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28145/28145 [==============================] - 730s 26ms/step - loss: 26.8848 - acc: 0.9814\n",
      "Epoch 80/250\n",
      "28145/28145 [==============================] - 729s 26ms/step - loss: 26.8843 - acc: 0.9819\n",
      "Epoch 81/250\n",
      "28145/28145 [==============================] - 730s 26ms/step - loss: 26.8846 - acc: 0.9811\n",
      "Epoch 82/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8839 - acc: 0.9822\n",
      "Epoch 83/250\n",
      "28145/28145 [==============================] - 730s 26ms/step - loss: 26.8837 - acc: 0.9820\n",
      "Epoch 84/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8840 - acc: 0.9816\n",
      "Epoch 85/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8832 - acc: 0.9826\n",
      "Epoch 86/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8832 - acc: 0.9823\n",
      "Epoch 87/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8827 - acc: 0.9830\n",
      "Epoch 88/250\n",
      "28145/28145 [==============================] - 730s 26ms/step - loss: 26.8825 - acc: 0.9828\n",
      "Epoch 89/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8828 - acc: 0.9827\n",
      "Epoch 90/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8818 - acc: 0.9835\n",
      "Epoch 91/250\n",
      "28145/28145 [==============================] - 730s 26ms/step - loss: 26.8825 - acc: 0.9827\n",
      "Epoch 92/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8816 - acc: 0.9836\n",
      "Epoch 93/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8816 - acc: 0.9834\n",
      "Epoch 94/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8814 - acc: 0.9835\n",
      "Epoch 95/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8815 - acc: 0.9838\n",
      "Epoch 96/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8810 - acc: 0.9838\n",
      "Epoch 97/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8810 - acc: 0.9837\n",
      "Epoch 98/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8806 - acc: 0.9843\n",
      "Epoch 99/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8804 - acc: 0.9844\n",
      "Epoch 100/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8804 - acc: 0.9843\n",
      "Epoch 101/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8805 - acc: 0.9841\n",
      "Epoch 102/250\n",
      "28145/28145 [==============================] - 734s 26ms/step - loss: 26.8802 - acc: 0.9844\n",
      "Epoch 103/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8800 - acc: 0.9843\n",
      "Epoch 104/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8797 - acc: 0.9849\n",
      "Epoch 105/250\n",
      "28145/28145 [==============================] - 733s 26ms/step - loss: 26.8797 - acc: 0.9847\n",
      "Epoch 106/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8793 - acc: 0.9851\n",
      "Epoch 107/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8796 - acc: 0.9848\n",
      "Epoch 108/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8790 - acc: 0.9854\n",
      "Epoch 109/250\n",
      "28145/28145 [==============================] - 731s 26ms/step - loss: 26.8789 - acc: 0.9856\n",
      "Epoch 110/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8789 - acc: 0.9853\n",
      "Epoch 111/250\n",
      "28145/28145 [==============================] - 730s 26ms/step - loss: 26.8787 - acc: 0.9854\n",
      "Epoch 112/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8783 - acc: 0.9860\n",
      "Epoch 113/250\n",
      "28145/28145 [==============================] - 741s 26ms/step - loss: 26.8785 - acc: 0.9856\n",
      "Epoch 114/250\n",
      "28145/28145 [==============================] - 727s 26ms/step - loss: 26.8782 - acc: 0.9859\n",
      "Epoch 115/250\n",
      "28145/28145 [==============================] - 736s 26ms/step - loss: 26.8782 - acc: 0.9858\n",
      "Epoch 116/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 26.8788 - acc: 0.9852\n",
      "Epoch 117/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8775 - acc: 0.9866\n",
      "Epoch 118/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8776 - acc: 0.9865\n",
      "Epoch 119/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8779 - acc: 0.9858\n",
      "Epoch 120/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8777 - acc: 0.9861\n",
      "Epoch 121/250\n",
      "28145/28145 [==============================] - 729s 26ms/step - loss: 26.8771 - acc: 0.9867\n",
      "Epoch 122/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 26.8771 - acc: 0.9865\n",
      "Epoch 123/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8771 - acc: 0.9867\n",
      "Epoch 124/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8769 - acc: 0.9867\n",
      "Epoch 125/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8766 - acc: 0.9873\n",
      "Epoch 126/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8765 - acc: 0.9872\n",
      "Epoch 127/250\n",
      "28145/28145 [==============================] - 738s 26ms/step - loss: 26.8768 - acc: 0.9867\n",
      "Epoch 128/250\n",
      "28145/28145 [==============================] - 734s 26ms/step - loss: 26.8765 - acc: 0.9870\n",
      "Epoch 129/250\n",
      "28145/28145 [==============================] - 735s 26ms/step - loss: 26.8761 - acc: 0.9876\n",
      "Epoch 130/250\n",
      "28145/28145 [==============================] - 734s 26ms/step - loss: 26.8763 - acc: 0.9872\n",
      "Epoch 131/250\n",
      "28145/28145 [==============================] - 734s 26ms/step - loss: 26.8757 - acc: 0.9881\n",
      "Epoch 132/250\n",
      "28145/28145 [==============================] - 736s 26ms/step - loss: 26.8759 - acc: 0.9877\n",
      "Epoch 133/250\n",
      "28145/28145 [==============================] - 732s 26ms/step - loss: 26.8760 - acc: 0.9876\n",
      "Epoch 134/250\n",
      "28145/28145 [==============================] - 736s 26ms/step - loss: 26.8756 - acc: 0.9879\n",
      "Epoch 135/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 26.8756 - acc: 0.9880\n",
      "Epoch 136/250\n",
      "28145/28145 [==============================] - 727s 26ms/step - loss: 26.8755 - acc: 0.9879\n",
      "Epoch 137/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8752 - acc: 0.9883\n",
      "Epoch 138/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8754 - acc: 0.9880\n",
      "Epoch 139/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8749 - acc: 0.9887\n",
      "Epoch 140/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8748 - acc: 0.9887\n",
      "Epoch 141/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8748 - acc: 0.9885\n",
      "Epoch 142/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8748 - acc: 0.9887\n",
      "Epoch 143/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8747 - acc: 0.9887\n",
      "Epoch 144/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8745 - acc: 0.9891\n",
      "Epoch 145/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8744 - acc: 0.9891\n",
      "Epoch 146/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8750 - acc: 0.9878\n",
      "Epoch 147/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8743 - acc: 0.9889\n",
      "Epoch 148/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8742 - acc: 0.9891\n",
      "Epoch 149/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8739 - acc: 0.9893\n",
      "Epoch 150/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8740 - acc: 0.9891\n",
      "Epoch 151/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8741 - acc: 0.9890\n",
      "Epoch 152/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8737 - acc: 0.9893\n",
      "Epoch 153/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8737 - acc: 0.9894\n",
      "Epoch 154/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8736 - acc: 0.9894\n",
      "Epoch 155/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8737 - acc: 0.9895\n",
      "Epoch 156/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8736 - acc: 0.9893\n",
      "Epoch 157/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8735 - acc: 0.9898\n",
      "Epoch 158/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8735 - acc: 0.9896\n",
      "Epoch 159/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8738 - acc: 0.9890\n",
      "Epoch 160/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8728 - acc: 0.9903\n",
      "Epoch 161/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8731 - acc: 0.9898\n",
      "Epoch 162/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8728 - acc: 0.9902\n",
      "Epoch 163/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8732 - acc: 0.9897\n",
      "Epoch 164/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8729 - acc: 0.9899\n",
      "Epoch 165/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8728 - acc: 0.9901\n",
      "Epoch 166/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8733 - acc: 0.9898\n",
      "Epoch 167/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8727 - acc: 0.9903\n",
      "Epoch 168/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8725 - acc: 0.9904\n",
      "Epoch 169/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8724 - acc: 0.9906\n",
      "Epoch 170/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8726 - acc: 0.9902\n",
      "Epoch 171/250\n",
      "28145/28145 [==============================] - 727s 26ms/step - loss: 26.8721 - acc: 0.9909\n",
      "Epoch 172/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8723 - acc: 0.9905\n",
      "Epoch 173/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8723 - acc: 0.9906\n",
      "Epoch 174/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8718 - acc: 0.9910\n",
      "Epoch 175/250\n",
      "28145/28145 [==============================] - 722s 26ms/step - loss: 26.8727 - acc: 0.9899\n",
      "Epoch 176/250\n",
      "28145/28145 [==============================] - 722s 26ms/step - loss: 26.8720 - acc: 0.9910\n",
      "Epoch 177/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8721 - acc: 0.9907\n",
      "Epoch 178/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8720 - acc: 0.9906\n",
      "Epoch 179/250\n",
      "28145/28145 [==============================] - 722s 26ms/step - loss: 26.8718 - acc: 0.9910\n",
      "Epoch 180/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8715 - acc: 0.9914\n",
      "Epoch 181/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8713 - acc: 0.9914\n",
      "Epoch 182/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8715 - acc: 0.9913\n",
      "Epoch 183/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8717 - acc: 0.9908\n",
      "Epoch 184/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8715 - acc: 0.9913\n",
      "Epoch 185/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8711 - acc: 0.9916\n",
      "Epoch 186/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8711 - acc: 0.9918\n",
      "Epoch 187/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8712 - acc: 0.9915\n",
      "Epoch 188/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8711 - acc: 0.9917\n",
      "Epoch 189/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8710 - acc: 0.9917\n",
      "Epoch 190/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8710 - acc: 0.9919\n",
      "Epoch 191/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8708 - acc: 0.9918\n",
      "Epoch 192/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8708 - acc: 0.9919\n",
      "Epoch 193/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8708 - acc: 0.9918\n",
      "Epoch 194/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8708 - acc: 0.9920\n",
      "Epoch 195/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8704 - acc: 0.9925\n",
      "Epoch 196/250\n",
      "28145/28145 [==============================] - 722s 26ms/step - loss: 26.8706 - acc: 0.9922\n",
      "Epoch 197/250\n",
      "28145/28145 [==============================] - 722s 26ms/step - loss: 26.8705 - acc: 0.9920\n",
      "Epoch 198/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8705 - acc: 0.9921\n",
      "Epoch 199/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8708 - acc: 0.9920\n",
      "Epoch 200/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8701 - acc: 0.9925\n",
      "Epoch 201/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8702 - acc: 0.9924\n",
      "Epoch 202/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8713 - acc: 0.9910\n",
      "Epoch 203/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8701 - acc: 0.9926\n",
      "Epoch 204/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8701 - acc: 0.9927\n",
      "Epoch 205/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8699 - acc: 0.9926\n",
      "Epoch 206/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8703 - acc: 0.9920\n",
      "Epoch 207/250\n",
      "28145/28145 [==============================] - 722s 26ms/step - loss: 26.8699 - acc: 0.9926\n",
      "Epoch 208/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8698 - acc: 0.9929\n",
      "Epoch 209/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8698 - acc: 0.9927\n",
      "Epoch 210/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8696 - acc: 0.9931\n",
      "Epoch 211/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8697 - acc: 0.9928\n",
      "Epoch 212/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8698 - acc: 0.9927\n",
      "Epoch 213/250\n",
      "28145/28145 [==============================] - 726s 26ms/step - loss: 26.8698 - acc: 0.9928\n",
      "Epoch 214/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8697 - acc: 0.9928\n",
      "Epoch 215/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8697 - acc: 0.9927\n",
      "Epoch 216/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8695 - acc: 0.9931\n",
      "Epoch 217/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8694 - acc: 0.9933\n",
      "Epoch 218/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8692 - acc: 0.9934\n",
      "Epoch 219/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8696 - acc: 0.9929\n",
      "Epoch 220/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8692 - acc: 0.9934\n",
      "Epoch 221/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8695 - acc: 0.9927\n",
      "Epoch 222/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8694 - acc: 0.9930\n",
      "Epoch 223/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8694 - acc: 0.9930\n",
      "Epoch 224/250\n",
      "28145/28145 [==============================] - 721s 26ms/step - loss: 26.8691 - acc: 0.9933\n",
      "Epoch 225/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8694 - acc: 0.9932\n",
      "Epoch 226/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8689 - acc: 0.9937\n",
      "Epoch 227/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8699 - acc: 0.9922\n",
      "Epoch 228/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8689 - acc: 0.9937\n",
      "Epoch 229/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8689 - acc: 0.9935\n",
      "Epoch 230/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8689 - acc: 0.9934\n",
      "Epoch 231/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8688 - acc: 0.9936\n",
      "Epoch 232/250\n",
      "28145/28145 [==============================] - 728s 26ms/step - loss: 26.8687 - acc: 0.9937\n",
      "Epoch 233/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8687 - acc: 0.9939\n",
      "Epoch 234/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8686 - acc: 0.9939\n",
      "Epoch 235/250\n",
      "28145/28145 [==============================] - 722s 26ms/step - loss: 26.8685 - acc: 0.9940\n",
      "Epoch 236/250\n",
      "28145/28145 [==============================] - 725s 26ms/step - loss: 26.8689 - acc: 0.9933\n",
      "Epoch 237/250\n",
      "28145/28145 [==============================] - 723s 26ms/step - loss: 26.8688 - acc: 0.9937\n",
      "Epoch 238/250\n",
      "28145/28145 [==============================] - 724s 26ms/step - loss: 26.8685 - acc: 0.9938\n",
      "Epoch 239/250\n",
      "28145/28145 [==============================] - 743s 26ms/step - loss: 26.8683 - acc: 0.9941\n",
      "Epoch 240/250\n",
      "28145/28145 [==============================] - 765s 27ms/step - loss: 26.8685 - acc: 0.9939\n",
      "Epoch 241/250\n",
      "28145/28145 [==============================] - 810s 29ms/step - loss: 26.8684 - acc: 0.9940\n",
      "Epoch 242/250\n",
      "28145/28145 [==============================] - 793s 28ms/step - loss: 26.8683 - acc: 0.9940\n",
      "Epoch 243/250\n",
      "28145/28145 [==============================] - 862s 31ms/step - loss: 26.8681 - acc: 0.9944\n",
      "Epoch 244/250\n",
      "28145/28145 [==============================] - 841s 30ms/step - loss: 26.8683 - acc: 0.9941\n",
      "Epoch 245/250\n",
      "28145/28145 [==============================] - 849s 30ms/step - loss: 26.8682 - acc: 0.9941\n",
      "Epoch 246/250\n",
      "28145/28145 [==============================] - 856s 30ms/step - loss: 26.8680 - acc: 0.9945\n",
      "Epoch 247/250\n",
      "28145/28145 [==============================] - 851s 30ms/step - loss: 26.8684 - acc: 0.9940\n",
      "Epoch 248/250\n",
      "28145/28145 [==============================] - 845s 30ms/step - loss: 26.8682 - acc: 0.9940\n",
      "Epoch 249/250\n",
      "28145/28145 [==============================] - 858s 30ms/step - loss: 26.8680 - acc: 0.9944\n",
      "Epoch 250/250\n",
      "28145/28145 [==============================] - 768s 27ms/step - loss: 26.8680 - acc: 0.9943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26314479320>"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csvlogger = CSVLogger('log/ner_bilstm_crf_dropout0.3.csv', append=True)\n",
    "model.fit(x_train, y_train, epochs=250, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28145/28145 [==============================] - 496s 18ms/step\n",
      "2399/2399 [==============================] - 48s 20ms/step\n",
      "Train set:\n",
      "precision: 0.9912629366977382 recall: 0.989065659803066 accuracy: 0.9997712391234671 f1: 0.9901630792542818\n",
      "CV set:\n",
      "precision: 0.8809965544659423 recall: 0.8648592392152782 accuracy: 0.997264169140377 f1: 0.8728533165274933\n"
     ]
    }
   ],
   "source": [
    "# set batch_size = 4096 will lead to OOM\n",
    "y_train_hat = model.predict(x_train, batch_size=2048, verbose=1)\n",
    "y_cv_hat = model.predict(x_cv, batch_size=2048, verbose=1)\n",
    "print('Train set:')\n",
    "eval(y_train, y_train_hat)\n",
    "print('CV set:')\n",
    "eval(y_cv, y_cv_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict        0     6     4     3     2     5     1\n",
      "Actual                                              \n",
      "0        1363816    80   175    99   188   110   538\n",
      "6             70  1466   106    21     4    12     0\n",
      "4            181    46  2976     4    46     0    25\n",
      "3            163    50     4  2526    24    90    19\n",
      "2            318     4   105    20  3727     2   217\n",
      "5            146    15     4    45     0  1073    47\n",
      "1            664     2    25     8    70    40  4852\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapping = np.array([0,6,4,3,2,5,1])\n",
    "y_true = y_cv\n",
    "y_pred = y_cv_hat\n",
    "y_t = np.argmax(y_true, axis=2).ravel()\n",
    "y_p = np.argmax(y_pred, axis=2).ravel()\n",
    "mat = pd.crosstab(y_t, y_p,rownames=['Actual'], colnames=['Predict'])\n",
    "mat = mat.loc[mapping,mapping]\n",
    "print(mat)\n",
    "plot_confusiopdmatrix(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_load_utils.load_all_weights(\n",
    "    model, r'model/ner_stacked_lstm_weights.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_obj('model/word_tokenizer.pickle', word_tokenizer)\n",
    "# save_obj('model/index_pos.pickle', index_pos)\n",
    "save_load_utils.save_all_weights(model, 'model/ner_stacked_lstm_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'O',\n",
       " 2: 'I-ORG',\n",
       " 3: 'I-LOC',\n",
       " 4: 'B-LOC',\n",
       " 5: 'I-PER',\n",
       " 6: 'B-ORG',\n",
       " 7: 'B-PER'}"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_pos = {v:k for k,v in pos_tokenizer.word_index.items()}\n",
    "index_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "近 O\n",
      "日 O\n",
      "， O\n",
      "位 O\n",
      "于 O\n",
      "江 B-ORG\n",
      "苏 I-ORG\n",
      "的 O\n",
      "江 B-ORG\n",
      "苏 I-ORG\n",
      "精 I-ORG\n",
      "研 I-ORG\n",
      "股 I-ORG\n",
      "份 I-ORG\n",
      "有 I-ORG\n",
      "限 I-ORG\n",
      "公 I-ORG\n",
      "司 I-ORG\n",
      "（ O\n",
      "以 O\n",
      "下 O\n",
      "简 O\n",
      "称 O\n",
      "“ O\n",
      "公 B-ORG\n",
      "司 I-ORG\n",
      "” O\n",
      "） O\n",
      "收 O\n",
      "到 O\n",
      "全 B-ORG\n",
      "资 I-ORG\n",
      "子 I-ORG\n",
      "公 I-ORG\n",
      "司 I-ORG\n",
      "精 O\n",
      "研 O\n",
      "（ O\n",
      "东 B-ORG\n",
      "莞 I-ORG\n",
      "） I-ORG\n",
      "科 I-ORG\n",
      "技 I-ORG\n",
      "发 I-ORG\n",
      "展 I-ORG\n",
      "有 I-ORG\n",
      "限 I-ORG\n",
      "公 I-ORG\n",
      "司 I-ORG\n",
      "（ O\n",
      "以 O\n",
      "下 O\n",
      "简 O\n",
      "称 O\n",
      "“ O\n",
      "东 B-LOC\n",
      "莞 I-LOC\n",
      "精 O\n",
      "研 O\n",
      "” O\n",
      "） O\n",
      "通 O\n",
      "知 O\n",
      "， O\n",
      "东 B-LOC\n",
      "莞 I-LOC\n",
      "精 O\n",
      "研 O\n",
      "变 O\n",
      "更 O\n",
      "了 O\n",
      "其 O\n",
      "经 O\n",
      "营 O\n",
      "范 O\n",
      "围 O\n",
      "， O\n",
      "履 O\n",
      "行 O\n",
      "了 O\n",
      "工 O\n",
      "商 O\n",
      "变 O\n",
      "更 O\n",
      "登 O\n",
      "记 O\n",
      "手 O\n",
      "续 O\n",
      "， O\n",
      "并 O\n",
      "取 O\n",
      "得 O\n",
      "了 O\n",
      "东 B-ORG\n",
      "莞 I-ORG\n",
      "市 I-ORG\n",
      "工 I-ORG\n",
      "商 I-ORG\n",
      "行 I-ORG\n",
      "政 I-ORG\n",
      "管 I-ORG\n",
      "理 I-ORG\n",
      "局 I-ORG\n",
      "换 O\n",
      "发 O\n",
      "的 O\n",
      "《 O\n",
      "营 O\n",
      "业 O\n",
      "执 O\n",
      "照 O\n",
      "》 O\n",
      "。 O\n"
     ]
    }
   ],
   "source": [
    "to_test = '大家好，我系渣渣辉，我系古天乐。'\n",
    "to_test = '国信证券股份有限公司（以下简称“国信证券”）作为北京网酒网电子商务股份有限公司（以下简称”公司”或“网酒网”）的持续督导主办券商。'\n",
    "to_test = '近日，位于江苏的江苏精研股份有限公司（以下简称“公司”）收到全资子公司精研（东莞）科技发展有限公司（以下简称“东莞精研”）通知，东莞精研变更了其经营范围，履行了工商变更登记手续，并取得了东莞市工商行政管理局换发的《营业执照》。'\n",
    "ner_ret = test_sentences(to_test)\n",
    "pretty_ner(to_test, ner_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# unit test for tokenizer sentences\n",
    "ss = [\n",
    "    '克马尔的女儿让娜今年读五年级，她所在的班上有30多名同学，该班的“家委会”由10名家长组成。', '其实非汉非唐，又是什么与什么呢？',\n",
    "    '当前国有大中型企业改制中存在的问题。'\n",
    "]\n",
    "vec = word_tokenizer.texts_to_sequences(ss)\n",
    "assert((x_train[5, -len(vec[0]):] == np.asarray(vec[0])).all())\n",
    "assert((x_cv[5, -len(vec[1]):] == np.asarray(vec[1])).all())\n",
    "assert((x_test[5, -len(vec[2]):] == np.asarray(vec[2])).all())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
